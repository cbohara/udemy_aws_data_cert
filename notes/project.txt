####
Initial setup
####
# Setup kinesis agent on an EC2 instance to send log files directly to kinesis firehose 

https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html
Linux AMI
sudo yum install â€“y aws-kinesis-agent
wget http://media.sundog-soft.com/AWSBigData/LogGenerator.zip
sudo mkdir /var/log/cadabra
cd /etc/aws-kinesis
sudo chmod a+rw agent.json
sudo vi agent.json
python ./LogGenderator.py 100

#####
Order history app
#####
Want a mobile user to be able to access their order history
Server logs > Kinesis data streams > Lambda > DynamoDB > mobile client

# Start kinesis stream CadabraOrders

# Create DynamoDB table
CadabraOrders
CustomerID = partition key = number
OrderId = sort key = string

# Setup Kinesis consumer to add rows to DynamoDB
ssh into EC2
install pip + setup AWS config
run Consumer.py

#####
Product recommendations model
#####
Offer users product recommendations via Spark ML
Server logs > Kinesis Firehose > S3 > EMR

# EMR
spin up cluster via console
3 nodes - 1 master, 2 core
specify Spark app

SSH into master node
cp /usr/lib/spark/examples/src/main/python/ml/als_example.py .
spark-submit als_example.py

update script using als-modifications.txt
specify S3 path where log data stored in S3

#####
Predict order quantities
#####
schema 
catagorial = represents different catagories
includes invoice number because it's not a quantative number
vs 
quantity = the number has actual meaning + can be used for regression analysis

######
Transaction rate alarm
######
kinesis agent sends order logs > kinesis data stream
analyze stream using kinesis data analytics
send anomoly events to another kinesis stream
triggers lambda function > SNS text message

Kinesis Data Analytics provisions capacity in the form of Kinesis Processing Units (KPU). A single KPU provides you with the memory (4 GB) and corresponding computing and networking. The default limit for KPUs for your application is eight.

#####
Near real-time log analysis
#####
server logs > kinesis firehose > elasticsearch service

######
data warehousing and visualization
######
server logs > firehose > S3 
S3 > glue > athena or redshift spectrum > quicksight

create IAM role for Redshift to access Glue data catalog + S3
add to cluster permissions

using redshift spectrum 
open editor in console
create an external schema to connect our redshift cluster with S3 data that is registered in the Glue data catalog

CREATE external schema database_name FROM data catalog
database 'database_name_in_glue'
iam_role 'iam_role_access_to_S3_and_Glue'
region 'us-east-1'
