#####
S3
#####
If your file is larger than 5GB, you must use multi-part upload
If your file is larger than 100MB, you should use it

Read after write consistency for PUTS of new S3 objects
As soon as the object is written, we can retrieve it
PUT 200 > GET 200
*Except if we did a GET before to see if the object exists
GET 404 > PUT 200 > GET 404 = eventually consistent

Eventual consistency for DELETES and PUTS of existing S3 objects
PUT 200 > PUT 200 (update) > GET 200 (older version of object)
DELETE 200 > GET 200 (eventually consistency = eventually delete)

####
S3 Storage Tiers
####
Make available while min cost

S3 Standard - General Purpose
Extremely high durability (11 9s)
Availability 4 9s (99.99%)
Sustain 2 concurrent facility failures - aka stored in 3 AZ
Use cases - big data analytics, mobiling + gaming apps, content distribution

S3 Reduced Redundancy Storage (RRS) 
Deprecated but may still be on exam
Durability 4 9s (99.99%)
Availability 4 9s (99.99%)
Sustain loss of data in a single facility  - akastored in 2 AZ
Use cases - noncritical, reproducible data at lower levels of redundancy 

S3 Infrequent Access (IA)
Data that is less frequently accessed, but requires rapid access when needed
Ex: Perform monthy analysis on some data = good use case vs perform analysis every 5 minutes = bad use case 
Extremely high durability (11 9s)
Sustain 2 concurrent facility failures - aka stored in 3 AZ
Low availability 99.9%
Low cost compared to S3 standard
Use as a data store for disaster recovery, backups

S3 One Zone - Infrequent Access (IA)
Same as IA but all data is stored in single AZ
Extremely high durability (11 9s) in single AZ, but data will be loss if AZ is destroyed 
Low availability 99.5%
Use case - secondary backup storing data you can recreate

S3 Intelligent Tiering (new!)
Small monthly monitoring + auto-tiering fee
Automatically moves objects between 2 access tiers based on changing access patterns
Extremely high durability (11 9s)
Availability 99.9%

S3 Glacier
Low cost object storage
Archiving/backup
Data retained for long term (10+ years)
Alt to on-prem magnetic tap storage
Extremely high durability (11 9s)
Cost per storage per month is very low
Each item in Glacier = Archive = up to 40 TB
Stored in Vaults (equivalent of bucket)
3 retreival options - fast = expensive, slow = cheaper


#####
S3 Lifecycle Rules
#####
Set of rules to move data between tiers to save storage costs 
Transition actions - defines when objects are transitioned to another storage class
Expiration actions - delete objects after a certain period of time
Move to glacier for helpful for backup

####
S3 versioning
####
Enabled at bucket level
Any time you overwrite a file you get a new version ID
The cost of versioning = the cost of the additional S3 stored or requested

Best practice to version your buckets
You can protect against unintended deletes (ability to restore a previous version)
Easy roll back to previous version
Any file that is not versioned prior to enabling versioning will have version "null"
You can suspend versioning

Ex: If you have a big data job that overwrites a file, and the job failed, you can always revert back to the previous correct version

####
S3 costs
####
no charge for data transferred within a region via COPY request
there is a charge for data transferred via COPY between reegions

no charge for data transferred between EC2 + S3 within same region
there is a cost between regions

after free tier, there is a cost for PUT and GET but not DELETE requests

Normal S3 pricing applies when your storage is accessed by another AWS account
Best to keep the dev and prod accounts in the same region to avoid unnecessary transfer costs
You can configure buckets as a Requester Pays bucket = requester account will be charged the cost of requests + downloads 

####
S3 cross-region replication
####
asynchronous replication between 2 buckets in sepregions
must enable versioning in source + dest bucket
must give proper IAM perms
can be in different accounts
applies to new objects
use case - compliance -  comply with stringent regulatory requirements for the storage of sensitive financial and personal data 
- big data analysis - if analysis is done by team in a distant country where the original bucket is

####
S3 ETag (Entity Tag)
####
how do you verify if a file has already been uploaded to S3?
names work, but how are you sure that the content of the file is exactly the same?

#####
S3 performance
#####
historically
greater than 100 TPS (transactions per second) > S3 performance degregation
suggested to add a random prefix to the file name to improve S3 partition distribution and therefore improve performance

now should only be required for extreme cases 
S3 can now support
write = 3,500 requests per second
read = 5,500 requests per second
https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/

increase PUT performance with multipart upload
must use for objects > 5GB
suggested to use for files > 100 MB
allows PUTS to be parallelized = greater throughput
max network bandwidth
if one upload of a part fails > decrease time to try to retry

increase GET performance with CloudFront
cache S3 objects around the world

S3 Transfer Acceleration
PUT file to node close to client and AWS will transfer file on their internal network to deliver to the destination S3 bucket 

If using SS3-KMS encryption for S3, you may have limited S3 GET capability because KMS is limited to ~100 - 1000 TPS (transactions per sec)

####
S3 Encryption for Objects
####
*important exam topic
4 methods

1. SSE-S3
SSE-S3 encrypts S3 objects using keys handled + managed by AWS S3
object encrypted server side
AES-256 encryption type
must set header as "x-amz-server-side-encryption":"AES256" when PUT object
then AWS will then create S3 managed data key and will be put in the bucket

2. SSE-KMS
SSE-KMS leverage AWS KMS (Key Management Service) to manage encryption keys
advantages of KMS - more control over rotation of the key + audit logs
must set header as "x-amz-server-side-encryption":"aws:kms" when PUT object
KMS Customer Master Key (CMK) created + now managed by KMS

SSE-C manage own encryption keys
fully managed by user outside of AWS - no keys stored within AWS
HTTPS must be used + provide data key in the header
AWS throws away the key right away

client side encryption - client handles encryption
need to use AWS Encryption Client
clients must encrypt data themselves before sending to S3
clients must decrypt data when retrieving data from S3

Encryption in transit (SSL)
S3 exposes
HTTP endpoint = non encrypted
HTTPS endpoint = encryption in flight = suggested
encryption in flight is also known as SSL/TLS

you can set default encryption for the bucket in the console

######
S3 security
######
S3 CORS (Cross-Origin Resource Sharing)
limit which websites can access files in your S3 buckets as specified in CORS config
This website is working when we go online, but when you run the website offline on your local host it doesn't work. Why? CORS

S3 Access Logs 
for audit purpose you may want to log all requests
will log in another S3 bucket + can query via Athena

User based
IAM policies - which API calls should be allowed for specific user from IAM console

Resource based
Bucket policies = bucket wide rules from S3 console - like allow cross account access
JSON based
resources = buckets + objeects
actions = allow or deny
principal = account/user to apply policy to
use case - grant public access to the bucket, force objects to be encrypted at upload, grant access from another account

Object Access Control List (ACL) finer grain
Bucket Access Control List (ACL) less common

What is the best way to ensure all objects in the bucket are encrypted?
old way = enable default encryption via bucket policy + refuse any HTTP command without proper headers
new way = use default encryption in S3
bucket policy trumps default encryption 

Other S3 security details
networking - supports VPC endpoints
logging + audit = S3 access logs or AWS CloudTrail
user security = MFA can be required for versioned buckets to delete objects
You can make an S3 object available to a user for a limited time period using signed URLs

#####
Glacier + Vault Policies
#####
low cost object storage for archiving + backup
alt to on-prem magnetic tape storage

each item in glacier = archive 
archives are stored in vaults

Use glacier any time you want to archive from S3 after X days 

each vault has 1 vault access policy + 1 vault lock policy
access policy = very similar to bucket policy - used for restricting user/account perms

lock policy = chose to lock the archive for regulatory or compliance requirements 
the policy is immutable aka can never be changed
ex: forbid deleting archive if less than 1 year old
ex: implement WORM (write once read many) policy - guarantee that the file cannot be overwritten later

###
S3 Select + Glacier Select
###
retrieve less data using SQL by performing server sidee filtering in advance
can filter by rows + columns (simple SQL statements)
result in less network transfer + less CPU cost client-side

S3 select can be used with Hadoop
transfer filtered data set from S3 before analyzing it with your EMR cluster

###
ElastiCache 
###
RDS = managed Redis or Memcached service
AWS takes care of all OS maintenance
caches are in-memory databases with really high performance + low latency
helps reduce load off of databases for read intensive workloads
helps make application stateless
write scaling using shards
read scaling using read replicas
multi AZ with failover capability

Redis overview
in memory key-value store
super low latency (sub ms)
cache survives reboot by default = persistence
more popular
use case - user sessions, leaderboard, distributed states, relieve pressure off databases, pub/sub capability for messaging

Memcached service
in-memory object store (like S3)
cache doesn't survive reboots
use cases- quick retreival of objects from memory
