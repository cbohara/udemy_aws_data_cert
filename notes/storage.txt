#####
S3
#####
If your file is larger than 5GB, you must use multi-part upload
If your file is larger than 100MB, you should use it

Read after write consistency for PUTS of new S3 objects
As soon as the object is written, we can retrieve it
PUT 200 > GET 200
*Except if we did a GET before to see if the object exists
GET 404 > PUT 200 > GET 404 = eventually consistent

Eventual consistency for DELETES and PUTS of existing S3 objects
PUT 200 > PUT 200 (update) > GET 200 (older version of object)
DELETE 200 > GET 200 (eventually consistency = eventually delete)

####
S3 Storage Tiers
####
Make available while min cost

S3 Standard - General Purpose
Extremely high durability (11 9s)
Availability 4 9s (99.99%)
Sustain 2 concurrent facility failures - aka stored in 3 AZ
Use cases - big data analytics, mobiling + gaming apps, content distribution

S3 Reduced Redundancy Storage (RRS) 
Deprecated but may still be on exam
Durability 4 9s (99.99%)
Availability 4 9s (99.99%)
Sustain loss of data in a single facility  - akastored in 2 AZ
Use cases - noncritical, reproducible data at lower levels of redundancy 

S3 Infrequent Access (IA)
Data that is less frequently accessed, but requires rapid access when needed
Ex: Perform monthy analysis on some data = good use case vs perform analysis every 5 minutes = bad use case 
Extremely high durability (11 9s)
Sustain 2 concurrent facility failures - aka stored in 3 AZ
Low availability 99.9%
Low cost compared to S3 standard
Use as a data store for disaster recovery, backups

S3 One Zone - Infrequent Access (IA)
Same as IA but all data is stored in single AZ
Extremely high durability (11 9s) in single AZ, but data will be loss if AZ is destroyed 
Low availability 99.5%
Use case - secondary backup storing data you can recreate

S3 Intelligent Tiering (new!)
Small monthly monitoring + auto-tiering fee
Automatically moves objects between 2 access tiers based on changing access patterns
Extremely high durability (11 9s)
Availability 99.9%

S3 Glacier
Low cost object storage
Archiving/backup
Data retained for long term (10+ years)
Alt to on-prem magnetic tap storage
Extremely high durability (11 9s)
Cost per storage per month is very low
Each item in Glacier = Archive = up to 40 TB
Stored in Vaults (equivalent of bucket)
3 retreival options - fast = expensive, slow = cheaper


#####
S3 Lifecycle Rules
#####
Set of rules to move data between tiers to save storage costs 
Transition actions - defines when objects are transitioned to another storage class
Expiration actions - delete objects after a certain period of time
Move to glacier for helpful for backup

####
S3 versioning
####
Enabled at bucket level
Any time you overwrite a file you get a new version ID
The cost of versioning = the cost of the additional S3 stored or requested

Best practice to version your buckets
You can protect against unintended deletes (ability to restore a previous version)
Easy roll back to previous version
Any file that is not versioned prior to enabling versioning will have version "null"
You can suspend versioning

Ex: If you have a big data job that overwrites a file, and the job failed, you can always revert back to the previous correct version

####
S3 costs
####
no charge for data transferred within a region via COPY request
there is a charge for data transferred via COPY between reegions

no charge for data transferred between EC2 + S3 within same region
there is a cost between regions

after free tier, there is a cost for PUT and GET but not DELETE requests

Normal S3 pricing applies when your storage is accessed by another AWS account
Best to keep the dev and prod accounts in the same region to avoid unnecessary transfer costs
You can configure buckets as a Requester Pays bucket = requester account will be charged the cost of requests + downloads 

####
S3 cross-region replication
####
asynchronous replication between 2 buckets in sepregions
must enable versioning in source + dest bucket
must give proper IAM perms
can be in different accounts
applies to new objects
use case - compliance -  comply with stringent regulatory requirements for the storage of sensitive financial and personal data 
- big data analysis - if analysis is done by team in a distant country where the original bucket is

####
S3 ETag (Entity Tag)
####
how do you verify if a file has already been uploaded to S3?
names work, but how are you sure that the content of the file is exactly the same?

#####
S3 performance
#####
historically
greater than 100 TPS (transactions per second) > S3 performance degregation
suggested to add a random prefix to the file name to improve S3 partition distribution and therefore improve performance

now should only be required for extreme cases 
S3 can now support
write = 3,500 requests per second
read = 5,500 requests per second
https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/

increase PUT performance with multipart upload
must use for objects > 5GB
suggested to use for files > 100 MB
allows PUTS to be parallelized = greater throughput
max network bandwidth
if one upload of a part fails > decrease time to try to retry

increase GET performance with CloudFront
cache S3 objects around the world

S3 Transfer Acceleration
PUT file to node close to client and AWS will transfer file on their internal network to deliver to the destination S3 bucket 

If using SS3-KMS encryption for S3, you may have limited S3 GET capability because KMS is limited to ~100 - 1000 TPS (transactions per sec)

####
S3 Encryption for Objects
####
*important exam topic
4 methods

1. SSE-S3
SSE-S3 encrypts S3 objects using keys handled + managed by AWS S3
object encrypted server side
AES-256 encryption type
must set header as "x-amz-server-side-encryption":"AES256" when PUT object
then AWS will then create S3 managed data key and will be put in the bucket

2. SSE-KMS
SSE-KMS leverage AWS KMS (Key Management Service) to manage encryption keys
advantages of KMS - more control over rotation of the key + audit logs
must set header as "x-amz-server-side-encryption":"aws:kms" when PUT object
KMS Customer Master Key (CMK) created + now managed by KMS

SSE-C manage own encryption keys
fully managed by user outside of AWS - no keys stored within AWS
HTTPS must be used + provide data key in the header
AWS throws away the key right away

client side encryption - client handles encryption
need to use AWS Encryption Client
clients must encrypt data themselves before sending to S3
clients must decrypt data when retrieving data from S3

Encryption in transit (SSL)
S3 exposes
HTTP endpoint = non encrypted
HTTPS endpoint = encryption in flight = suggested
encryption in flight is also known as SSL/TLS

you can set default encryption for the bucket in the console

######
S3 CORS (Cross-Origin Resource Sharing)
######

