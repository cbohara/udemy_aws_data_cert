######
Lambda
######
in the world of big data - used to process data as it is moved around from one service to another
lambda can be triggered by time like cron but primarily event driven
lambda must be in the same account as the service triggering it, in addition to having an IAM policy granting it access
if your code does fail for some reason lambda will automatically retry 3 times
https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-max-retry-attempts-event-age-asynchronous-invocations/

######
Kinesis > Lambda
######
kinesis is not pushing data to lambda
lambda polls for data from kinesis stream periodically in batches 
max batch size is 10,000 records
too large of a batch size can cause timeouts (max timeout is 15 minutes)
max batch size is 6MB

if lambda fails it will retry until it succeeds or the data expires
this can stall the shards if you don't handle errors properly
this is because the kinesis stream is just waiting for the lambda to complete before proceeding 
aka lambda processes shard data synchronously 

#####
Lambda Example
#####
need to base64 decode incoming kinesis data records

#####
Glue
#####

#######
Glue crawler + catalog
#######
s3 partitions
make sure your data is structured by how you want to query the data

glue crawler 
discovery + publish S3 schema so it can be used for analysis tools (Athena, Redshift, EMR) 
also registers partitions

glue crawler publishes schema in glue catalog

glue catalog
serves as a central metadata repository for data lake in S3
stores where the data exists in s3, what the schema of the data is, how it is partitioned, etc
can provide metadata to Hive on EMR

when you create tables and databases manually, Athena uses HiveQL data definition language (DDL) statements 
such as CREATE TABLE, CREATE DATABASE, and DROP TABLE
under the hood to create tables and databases in the AWS Glue Data Catalog

#######
Glue ETL jobs 
#######
build custom ETL serverless Spark jobs > transform, clean, and enrich data before doing analysis
can also automatically generate code in Python (or Scala) to perform ETL 
encryption features - SSE at rest, SSL in transit
can be event-driven via triggers, not just on a schedule
can provision additional DPUs (data processing units) if underperforming
errors reported to cloudwatch
format conversions - CSV, JSON, Avro, Parquet 

built in transformations
DropFields, DropNullFields - perform cleanup by dropping null fields
Filter - specify a function to filter records 
Join - enrich data from another table
Map - add/delete fields beyond a join

ML transformations
FindMatches ML
used to ID duplicate or matching records in the data set
even when the records do not have a common unique identifier and no fields match exactly

can also use built-in Spark transformations
including K-Means machine learning library

development endpoints
can develop ETL in a notebook
then create a glue job to run the script

endpoint is in a VPC controlled by security groups 
can use elastic IPs to access a private endpoint address
connect via Zeppelin running on local machine
Zeppelin notebook server running on EC2
can use Sagemaker notebooks
can open terminal window (my method)
PyCharm

running glue jobs
scheduler - like cron
job bookmarks - allows you to pick up where you left off in the last job run
persists the state of the job run
avoids processing old data
works best with keeping track of where you left off in S3
can integrate with Cloudwatch events so after Glue job completed > trigger SNS/Lambda
can invoke EC2 run, send event to kinesis stream, or activate step function in AWS Data Pipeline

#######
Glue cost + antipatterns
#######
billed by the minute for crawler + ETL jobs 
first million objects stored are free
development endpoint is charged per minute

anti-patterns
- should not be used for real-time processing
supposed to be used for batch processing

- shouldn't be used with DynamoDB because it is for fast lookups + requires no schema
Glue is all about defining schema over S3

- better off using EMR or AWS Data Pipeline if working with other engines already

######
EMR
######
managed Hadoop framework on EC2 instances

Hadoop
- HDFS
- YARN
- MapReduce

Architecture
1 master/leader node
manage cluster 
runs all software compontents required to coordinate distribution of data + distribution of tasks

core nodes
run tasks that analyze data
store data in HDFS

task nodes
only run tasks 
do not store data
good use of spot instances
scale up + down as needed

Transient vs Long-running clusters
transient cluster = only use capacity you need
perform a bunch of steps 
store results 
shutdown cluster

long-running cluster
use cluster to store data as a data warehouse
enable termination + autotermination protection

EMR/AWS integration
let Amazon manage hosts on EC2
can run cluster in VPC for security purposes
can use S3 to store input + output data
integrate with CloudWatch to monitor performance
can use with CloudTrail to audit requests
integrate with AWS Data Pipeline - can spin up + shut down transient EMR cluster as part of the pipeline

######
EMR Storage
######
HDFS
stores data across different instances in the cluster
try to analyze data on the instance where the data is stored
stores multiple copies of data on different instances
if you have a large file stored in HDFS, that file is going to be broken into blocks + those blocks are going to be stored in multiple places for backup purposes
default block size is 128MB = ideal size to break your files into for sstorage in HDFS 
HDFS is ephemeral = when you terminate the cluster the data is gone

EMRFS = file system that looks like HDFS but is backed by S3

EMRFS consistent view
if you have a bunch of different nodes trying to write or read from S3 at the same time > consistency problem
behind the hood uses DynamoDB database to store S3 metadata

local file system is also available for storage
useful for storing temporary data like buffers + caches

EBS for HDFS 
AWS automatically attach general purpose SSD 10 GB volume as root device
used to enhace performance as root volume
you can only attach EBS volumes when launching a cluster, not if the cluster is already running
you cannot remove an EBS volume after launching

#####
EMR promises
#####
EMR charges by the hour + EC2 charges 
EMR will provision new nodes if core nodes failYou can add and remove task nodes on the fly
You can resize running cluster's core nodes

#####
Apache Spark
#####
uses in-memory caching
uses DAGs to optimize query execution
are not meant for OLTP or batch processing
meant for analytical applications or doing larger tasks on a regular schedule

Spark context object = driver program/script
actual code
will use YARN as cluster manager in EMR to determine how to distribute the work and aquires executor nodes to perform the work
executor nodes = processes that run computations and store the data for the application
the application code is sent to each executor
spark context sends the tasks for the executor to run

Spark core = foundation of the platform

Spark SQL contains dataset construct
makes it easy to import data as if it is a giant database
allows you to run SQL queries against thee dataset

Spark streaming for streaming analytics
ingests data in mini batches 
can use the same application code you would use for conventional Spark batch analytics
so it improves developer productivity

Other spark libs:
MLLib 
GraphX

Spark Structured Streaming
use dataset = like database table
can imagine spark streaming = database table that keeps growing forever
can query data over windows of time

Spark Streaming + Kinesis
can import data into Spark Streaming from Kinesis Data Stream

Spark +  Redshift
spark-redshift package makes it easy to query Redshift from spark job
useful for ETL using Spark

########
AWS Data Pipeline
########
web service for reliably processing + moving data between diff AWS compute + storage services
task scheduling framework  where you can run different operations that depend on each other at different schedules

ex: log files from EC2 > S3 > analyze via EMR
can orchestrate with data pipeline

destinations - S3, RDS, DynamoDB, Redshift + EMR
manages task dependencies
will auto retry (default 3 up to 10) + notify via on failure alarm
pipelines can work across regions
can add pre-condition checks - either built in or execute custom script via bash command
data sources may be on prem

activities = action - EMR, Hive, Copy, SQL, scripts

########
AWS Step Functions
########
used to design workflows
easy visualization
advanced error handling + retry mechanism outside the code
can maintain audit history of workflows
wait between steps
max execution time of a state machine is  1 year = can handle long lived processes/workflows
defined using JSON based Amazon Steps Languagee (ASL) 

can also be used to monitor a single job using  built in error handling + retry

https://stackshare.io/stackups/aws-data-pipeline-vs-aws-step-functions
AWS Data Pipeline can be classified as a tool in the "Data Transfer" category
AWS Step Functions is grouped under "Cloud Task Management"

https://stackoverflow.com/questions/55061621/aws-data-pipeline-vs-step-functions
Data Pipeline specializes for import/output with AWS data storage resources
Step Functions are good candidate tying together lambda jobs
