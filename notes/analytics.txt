#####
Kinesis analytics
#####
not only accepts input from kinesis data stream
also accepts input from kinesis firehose

can also include reference source table to enrich your input data stream
store reference data as an object in S3
creates in-app table for lookups

application code = SQL statements 
operates over windows of time

destination > kinesis data stream or firehose
any errors that occur will be sent to kinesis error stream

use cases for Kinesis Analytics
1. realtime ETL
2. continuous metric generation
ex: calculate # of uniq website visitors every 5 minutes
send results to Redshift for further analysis 
3. responsive analytics 
ex: monitor for events that mean certain criteria + then automatically notify the right customers > output to kinesis stream > notify via SNS

cost
use carefully bc it isn't cheap

security
enabled via iam permissions

schema discovery 
analyzes incoming data from your stream
edit + correct as needed

RANDOM_CUT_FOREST
SQL function
used for anomaly detection on numeric columns in a stream
novel way to identify outliers in a data set

https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-app-code.html
when you select rows, you insert results into another in-application stream
use pumps to write to an in-application stream

#####
AWS Elasticsearch Service
#####
petabyte scale analysis and reporting
horizontally scalable
performant and cheap

elasticsearch is a search engine
built on top of open source version of Lucine
part of the elastic stack

includes Beats/LogStash - allows you to import data at scale into ElasticSearch cluster
can use Kibana to visualize the data - like a Google Analytics dashboard

use cases
full-text search
log analytics
application monitoring
security analytics
clickstream analytics

ElasticSearch concepts
documents - things you are looking for
can be more than text - any structured JSON data works
every document has a unique ID

types - define schema + mapping shared by documents that represent the same sort of thing
going to be deprecated and replaced with indices

indices - power search into all documents within a collection of types
contain inverted indices that let you search across everything within them at one
essentially a collection of documents that are related to some thing

indices are split into shards
each shard exists in a different node in a cluster
each shard is its own mini search engine

each index has 2 primary shards + 2 replicas
write requests are routed to the primary shard, then replicated
read requests are routed to the primary or any replica
your application is responsiblef for round-robin the read requests amongst the nodes

Amazon ElasticSearch service
fully-managed but not serverless
you need to chose the number of nodes like EMR
pay for what you use - instance hours, storage, and data transfer
network isolation using VPC
security with encryption at rest + in transit
control access via cognito and IAM

integration with other AWS services
itegrates well with IoT
offers node awareness - can have 2 AZ in same region = increase availability but increase latency
can use kinesis > lambda > elasticsearch
can also receieve dynamoDB streams

setup options
how many dedicated master nodes do you want? 
domains aka cluster
snapshots to S3? good for automated backup storage
zone awareness? increase availability but also increase latency 

ES Security
request signing required for all incoming traffic
can setup cluster in the VPC but cannot change later

if you setup ES cluster in a VPC, how do you access Kibana (which is a web interface)?
easiest way to do so is to use Cognito for login > access Kibana
if working on public you can update access policy to specify your IP address

ES antipatterns
shouldn't be used for OLTP (no transaction support)
encouraged for search + analytics but not necessarily ad-hoc querying
AWS encourages you to use AWS for that

######
Athena
######
ORC and Parquet are columnar and splittable
Avro is splittable but more for row based storage

use cases
good for ad-hoc querying
can checkout data before loading into Redshift data warehouse
analyze logs 
integration with Jupyter/Zeppelin
integration with QuickSight or via ODBC/JDBC with other viz tools

Athena + Glue
Glue stores metadata about S3 dataset
expose table definition to Athena

Athena cost model
$5/TB scanned for successful or cancelled queries
not charged for failed queries
no charge for DDL (create/alter/drop)
better cost + performance with ORC and Parquet
partitioning data reduces cost > scan less data

security
can encrypt Athena results using SSE-SE, SSE-KMS, or CSE-KMS
transport layer security (TLS) encrypts in-transit between Athena and S3

#######
Redshift
#######
fully managed petabyte scale data warehouse service
fast using machine learning, massively parallel query execution, columnar storage, and high performance disks
designed for OLAP not OLTP
use ODBC and JDBC interfaces for SQL executions
can scale up + down on demand
use replication + continuous backups
automatically recovery from node failures

use cases
accelerate analytics workloads
unified data warehouse + data lake

# Architecture
leader node
interface for external clients
develops execution plan = ordered set of steps to process query
coordinates parallel execution of plans with compute nodes
aggregates intermediate results from the nodes
returns the results back to the application

compute nodes
each cluster can have 1-128 nodes
responsible for executing steps provided by leader node
sends intermediate results back to leader node for aggregation
has its own dedicated CPU, memory, attached disk storage

2 diff compute nodes
1. dense storage (DS)
create a very large data warehouse using hard drive disks (HDDs) at a very low price point
2. dense compute (DC)
high performance using fast CPUs and solid state disks (SSD)

every compute node is divided into slices
# of slices per node is determined by node size of the cluster
process chunks of the data given to it

# Redshift Spectrum
query exabytes of unstructured data in S3 without loading into Redshift cluster
limitless concurrency
horizontal scaling
separate storage + compute resources
spectrum is just taking care of the compute
supports Gzip and snappy compression

# Redshift performance
Massive Parallel Processing (MPP)

columnar data storage -
efficient because all values of the same data type can be stored more efficiently vs row based storage in which each column has a different data type
block size of 1 MB
https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html

column compression -
cannot change after column is created
can analyze the logic used
can have it taken care of automatically or explicitly define

# Redshift durability
data loaded into redshift is stored in 3 places
1. original copy
2. replica copy within the cluster - if your cluster has more than 1 compute node
3. periodic backups into S3 - asychronously replicated in another region for disaster recovery

in addition you can enable automated snapshots of the entire cluster
defaults to 1 day retention period but can be up to 35 days 
if cluster fails > create a new cluster and restore with snapshot

note a redshift cluster is limited to a single AZ

# Redshift scaling
can perform vertical (increasing node instance type) + horizontal scaling (increasing the number of nodes) on demand
how does this work?
existing redshift cluster becomes read only
new cluster created
when new cluster is ready, the existing cluster will be temp unavailable while CNAME is flipped for the new cluster 
then data will be moveed from the compute nodes in the existing data warehouse cluster in parallel to the compute nodes in the new cluster

# Redshift distribution styles
chosen upon table creation

2 goals for data distribution 
1. distribute the workload uniformly among the nodes
2. minimize data movement during query execution

AUTO
automatically figured out by Redshift

EVEN
round robin
not trying to cluster together info in any logical sort of way

KEY
distributed according to values in 1 column
good when querying based on specific column in your data
ex: user ID 

ALL
copy of the entire table is distributed on every node
takes much longer to load the data
only appropriate for slow moving tables that aren't updated frequently
not good for small dimension tables

# Redshift sort keys
similar to indexes or indices in RDBS
values in the column are stored on disk in sorted order
enables efficient handling of range restricted predicates
stores min + max values on block as part of its metadata
allows you to skip over ranges of data when querying

what's a good choice of sort key?
if you are more likely to query more recent data frequently = use timestamp
frequent range or equality filtering on a specific column = use that column
frequently join table = use column you are joining on

single sort key
single value to sort the data
good for querying by a single column

compound sort key
contains many columns 
order matters 
primary column = used most often
default sort type
improves compression

interleaved sort key
equal weight to each column
useful if multiple queries use different columns for filters
