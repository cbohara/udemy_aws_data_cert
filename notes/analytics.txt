#####
Kinesis analytics
#####
not only accepts input from kinesis data stream
also accepts input from kinesis firehose

can also include reference source table to enrich your input data stream
store reference data as an object in S3
creates in-app table for lookups

application code = SQL statements 
operates over windows of time

destination > kinesis data stream or firehose
any errors that occur will be sent to kinesis error stream

use cases for Kinesis Analytics
1. realtime ETL
2. continuous metric generation
ex: calculate # of uniq website visitors every 5 minutes
send results to Redshift for further analysis 
3. responsive analytics 
ex: monitor for events that mean certain criteria + then automatically notify the right customers > output to kinesis stream > notify via SNS

cost
use carefully bc it isn't cheap

security
enabled via iam permissions

schema discovery 
analyzes incoming data from your stream
edit + correct as needed

RANDOM_CUT_FOREST
SQL function
used for anomaly detection on numeric columns in a stream
novel way to identify outliers in a data set

https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works-app-code.html
when you select rows, you insert results into another in-application stream
use pumps to write to an in-application stream

#####
AWS Elasticsearch Service
#####
petabyte scale analysis and reporting
horizontally scalable
performant and cheap

elasticsearch is a search engine
built on top of open source version of Lucine
part of the elastic stack

includes Beats/LogStash - allows you to import data at scale into ElasticSearch cluster
can use Kibana to visualize the data - like a Google Analytics dashboard

use cases
full-text search
log analytics
application monitoring
security analytics
clickstream analytics

ElasticSearch concepts
documents - things you are looking for
can be more than text - any structured JSON data works
every document has a unique ID

types - define schema + mapping shared by documents that represent the same sort of thing
going to be deprecated and replaced with indices

indices - power search into all documents within a collection of types
contain inverted indices that let you search across everything within them at one
essentially a collection of documents that are related to some thing

indices are split into shards
each shard exists in a different node in a cluster
each shard is its own mini search engine

each index has 2 primary shards + 2 replicas
write requests are routed to the primary shard, then replicated
read requests are routed to the primary or any replica
your application is responsiblef for round-robin the read requests amongst the nodes

Amazon ElasticSearch service
fully-managed but not serverless
you need to chose the number of nodes like EMR
pay for what you use - instance hours, storage, and data transfer
network isolation using VPC
security with encryption at rest + in transit
control access via cognito and IAM

integration with other AWS services
itegrates well with IoT
offers node awareness - can have 2 AZ in same region = increase availability but increase latency
can use kinesis > lambda > elasticsearch
can also receieve dynamoDB streams

setup options
how many dedicated master nodes do you want? 
domains aka cluster
snapshots to S3? good for automated backup storage
zone awareness? increase availability but also increase latency 

ES Security
request signing required for all incoming traffic
can setup cluster in the VPC but cannot change later

if you setup ES cluster in a VPC, how do you access Kibana (which is a web interface)?
easiest way to do so is to use Cognito for login > access Kibana
if working on public you can update access policy to specify your IP address

ES antipatterns
shouldn't be used for OLTP (no transaction support)
encouraged for search + analytics but not necessarily ad-hoc querying
AWS encourages you to use AWS for that

######
Athena
######
ORC and Parquet are columnar and splittable
Avro is splittable but more for row based storage

use cases
good for ad-hoc querying
can checkout data before loading into Redshift data warehouse
analyze logs 
integration with Jupyter/Zeppelin
integration with QuickSight or via ODBC/JDBC with other viz tools

Athena + Glue
Glue stores metadata about S3 dataset
expose table definition to Athena

Athena cost model
$5/TB scanned for successful or cancelled queries
not charged for failed queries
no charge for DDL (create/alter/drop)
better cost + performance with ORC and Parquet
partitioning data reduces cost > scan less data

security
can encrypt Athena results using SSE-SE, SSE-KMS, or CSE-KMS
transport layer security (TLS) encrypts in-transit between Athena and S3

#######
Redshift
#######
fully managed petabyte scale data warehouse service
fast using machine learning, massively parallel query execution, columnar storage, and high performance disks
designed for OLAP not OLTP
use ODBC and JDBC interfaces for SQL executions
can scale up + down on demand
use replication + continuous backups
automatically recovery from node failures

use cases
accelerate analytics workloads
unified data warehouse + data lake

# Architecture
leader node
interface for external clients
develops execution plan = ordered set of steps to process query
coordinates parallel execution of plans with compute nodes
aggregates intermediate results from the nodes
returns the results back to the application

compute nodes
each cluster can have 1-128 nodes
responsible for executing steps provided by leader node
sends intermediate results back to leader node for aggregation
has its own dedicated CPU, memory, attached disk storage

2 diff compute nodes
1. dense storage (DS)
create a very large data warehouse using hard drive disks (HDDs) at a very low price point
2. dense compute (DC)
high performance using fast CPUs and solid state disks (SSD)

every compute node is divided into slices
# of slices per node is determined by node size of the cluster
process chunks of the data given to it

# Redshift Spectrum
query exabytes of unstructured data in S3 without loading into Redshift cluster
limitless concurrency
horizontal scaling
separate storage + compute resources
spectrum is just taking care of the compute
supports Gzip and snappy compression

# Redshift performance
Massive Parallel Processing (MPP)

columnar data storage -
efficient because all values of the same data type can be stored more efficiently vs row based storage in which each column has a different data type
block size of 1 MB
https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html

column compression -
cannot change after column is created
can analyze the logic used
can have it taken care of automatically or explicitly define

# Redshift durability
data loaded into redshift is stored in 3 places
1. original copy
2. replica copy within the cluster - if your cluster has more than 1 compute node
3. periodic backups into S3 - asychronously replicated in another region for disaster recovery

in addition you can enable automated snapshots of the entire cluster
defaults to 1 day retention period but can be up to 35 days 
if cluster fails > create a new cluster and restore with snapshot

note a redshift cluster is limited to a single AZ

# Redshift scaling
can perform vertical (increasing node instance type) + horizontal scaling (increasing the number of nodes) on demand
how does this work?
existing redshift cluster becomes read only
new cluster created
when new cluster is ready, the existing cluster will be temp unavailable while CNAME is flipped for the new cluster 
then data will be moveed from the compute nodes in the existing data warehouse cluster in parallel to the compute nodes in the new cluster

# Redshift distribution styles
chosen upon table creation

2 goals for data distribution 
1. distribute the workload uniformly among the nodes
2. minimize data movement during query execution

AUTO
automatically figured out by Redshift

EVEN
round robin
not trying to cluster together info in any logical sort of way

KEY
distributed according to values in 1 column
good when querying based on specific column in your data
ex: user ID 

ALL
copy of the entire table is distributed on every node
takes much longer to load the data
only appropriate for small slow moving tables that aren't updated frequently

# Redshift sort keys
similar to indexes or indices in RDBS
values in the column are stored on disk in sorted order
enables efficient handling of range restricted predicates
stores min + max values on block as part of its metadata
allows you to skip over ranges of data when querying

what's a good choice of sort key?
if you are more likely to query more recent data frequently = use timestamp
frequent range or equality filtering on a specific column = use that column
frequently join table = use column you are joining on

single sort key
single value to sort the data
good for querying by a single column

compound sort key
contains many columns 
order matters 
primary column = used most often
default sort type
improves compression

interleaved sort key
equal weight to each column
useful if multiple queries use different columns for filters

# Import / Export
COPY 
parallelized, efficient import external data into Redshift cluster
can specify prefix to copy all files under
also provide specific manifest file
authorize via IAM roles or AWS key + secrete key 
can decrypt data that is encrypted in S3 while COPYing 

not a good idea to run COPY commands in parallel
better to split your data into separate files no more than 1GB apiece and compress
then load each file sequentially 

if you need to load a "narrow" table (lots of rows, few columns) = use single COPY command

INSERT INTO SELECT
move data within Redshift

CREATE TABLE AS
refer to data that is already in Redshift in another table

UNLOAD
unload table into files into S3

Enhanced VPC routing
copy data within VPC rather than public internet

# Cross-region replication for backup
say you want to copy a snapshot into another region for backup purposes
and the original Redshift cluster uses KMS-encryption

in destination region
create a KMS key if you don't have one
setup name for snapshot copy grant
associate KMS key with copy grant

in source region
enable copying of snapshots to the copy grant in the destination region

DBLINK
extension that allows you to connect Redshift cluster to PostgreSQL
copy + sync data 
allows you to take advantage of best of both worlds - columnar and row based storage

Redshift integrates with 
S3 - import + export 
DynamoDB - import data
EMR/EC2 - import via SSH 
Data Pipeline - automate data movement in + out of redshift
Database Migration Service - helps with import process from existing data warehouse

# Redshift Workload Management (WLM)
prioritize short, fast queries over long, slow queries 
setup different query queues
setup via console, CLI, or API

# concurrency scaling
automatically add cluster capacity to handle increase in concurrent read queries
support virtually unlimited concurrent users + queries
use WLM queues to managee which queries are sent to the concurrency scaling cluster
prioritize queries that need to be done asap to take advantage of concurrency scaling while allow others to take their time with standard resources

# automatic workload management options
creates up to 8 queues (default 5)
large queries = low concurrecy = more resources available per query
small queries = high concurrency = less resources per query

configuring queues by
- priority - dedicated resources for specific higher prio queries 
- concurrency scaling - enable more resources for bursts in read queries
- user groups - segregate queries by users
- query groups - segregate by automated processes
- query monitoring rules
ex: abort queries that take longer than 60 seconds if they are in the small query queue + move it to a longer query queue

# manual workload management
1 default queue that can handle 5 concurrent queries
superuser queue with concurrency level of 1 = for admin queries from DBA
define up to 8 queues with concurrency level up to 50

# short query acceleration (SQA)
prioritize short running queries over long running ones
can be used in place of WLM
can work with CREATE TABLE AS (CTAS) or read-only SELECT queries
use machine learning algo to determine run time
can configure how many seconds you consider "short"

VACUUM command
recovers space from deleted rows + restores sort order
VACUUM FULL = default = 
VACUUM DELETE ONLY = deletes only, no resortingg rows
VACUUM SORT ONLY = sort only but not reclaim disk space
VACUUM REINDEX = used for cleanup for tables using interleaved sort key option

Redshift anti-patterns
Small data sets - use RDS instead
OLTP - use RDS or DynamoDB instead
Unstructured data - ETL first + then load into Redshift
BLOB data - better use S3 + store references to files in S3

# Resizing redshift clusters
Elastic resize
quickly add/remove nodes of the same type
cluster is only down for a few minutes
tries to keep connections open during down time
limited to doubling or halving for certain node types 

Classic resize
changing node types and/or number of nodes 
cluster is read-only for hours to days

Better alternative to classic resize
snapshot to copy cluster
resize new cluster
shift traffic to new cluster from older cluster to new cluster

new redshift features for 2020
RA3 nodes with managed storage = scale compute + storage capacity independently
Redshift data lake export  = unload Redshift queries to S3 in Apache Parquet format with auto partitioning

Redshift spectrum vs athena
https://stackoverflow.com/questions/50250114/athena-vs-redshift-spectrum
Redshift spectrum is good if you already have a Redshift cluster up + running
allows you to join data residing in Redshift with data residing in S3 without having to load the data in  

tips from AWS reinvent workshop
5 points of guidance = SET DW
S
sort key
improve filter performance
choose up to 3 columns

E
encoding of columns
compress all columns except for 1st sort key column

T
table maintainance
auto vacuum

D
distribution key

W
warehouse management
setup different queues = one for ETL, one for ad-hoc
can setup query monitoring rules
ex: stops user from running select *

#####
RDS
#####
hosted relational database
more relevant for small data

ACID compliance
Atomicity
entire transaction as a whole has to be successful
if any parts fails the transaction is invalidated
Consistency
must adhere to all defined rules (ex: constraints)
Isolation 
each transaction is independent 
necessary for concurrency control
Durable
once transaction is successfully completed the data is guaranteed to be stored 

Amazon Aurora
compatible with MySQL and PostgreSQL
up to 5x faster than MySQL and 3x faster than PostgreSQL
stores up to 64TB per database instance
Up to 15 read replicas
continuous backup to S3
replication across AZ
automatic scaling with Aurora Serverless

security
VPC network isolation
at rest encryption with KMS
in transit encryption with SSL
